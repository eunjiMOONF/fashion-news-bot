# -*- coding: utf-8 -*-
import requests
import json
from datetime import datetime, timedelta
import os
import time
from bs4 import BeautifulSoup
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FashionNewsBot:
    def __init__(self):
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ë“¤ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ì§ì ‘ ì„¤ì •ëœ ê°’ ì‚¬ìš©)
        self.naver_client_id = os.getenv('NAVER_CLIENT_ID') or "pPilWOLS9m2zOgfbQ24A"
        self.naver_client_secret = os.getenv('NAVER_CLIENT_SECRET') or "mPg8xJxCPy"
        self.slack_webhook_url = os.getenv('SLACK_WEBHOOK_URL') or "https://hooks.slack.com/services/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"
        
        # ë””ë²„ê¹…ìš© ì¶œë ¥
        print(f"ğŸ”‘ NAVER_CLIENT_ID: {'âœ… ì„¤ì •ë¨' if self.naver_client_id else 'âŒ ì—†ìŒ'}")
        print(f"ğŸ”‘ NAVER_CLIENT_SECRET: {'âœ… ì„¤ì •ë¨' if self.naver_client_secret else 'âŒ ì—†ìŒ'}")
        print(f"ğŸ”‘ SLACK_WEBHOOK_URL: {'âœ… ì„¤ì •ë¨' if self.slack_webhook_url else 'âŒ ì—†ìŒ'}")
        
        # íŒ¨ì…˜ ê´€ë ¨ í‚¤ì›Œë“œë“¤ (ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬)
        self.fashion_keywords = [
            'K-íŒ¨ì…˜', 'íŒ¨ì…˜íŠ¸ë Œë“œ', 'íŒ¨ì…˜ë¸Œëœë“œ', 'íŒ¨ì…˜ì‚°ì—…', 'íŒ¨ì…˜ê¸°ì—…',
            'ì˜ë¥˜ì‚°ì—…', 'íŒ¨ì…˜ì‹œì¥', 'íŒ¨ì…˜ë””ìì¸', 'ì„¬ìœ ì‚°ì—…', 'íŒ¨ì…˜í…Œí¬', 
            'íŒ¨ì…˜í”Œë«í¼', 'íŒ¨ì…˜ìŠ¤íƒ€íŠ¸ì—…', 'í•œêµ­íŒ¨ì…˜', 'íŒ¨ì…˜ìœ„í¬', 
            'ëª…ë™íŒ¨ì…˜', 'ë™ëŒ€ë¬¸íŒ¨ì…˜', 'íŒ¨ì…˜ìœ í†µ', 'ì˜¨ë¼ì¸íŒ¨ì…˜', 
            'íŒ¨ì…˜ì†Œë¹„', 'ì§€ì†ê°€ëŠ¥íŒ¨ì…˜', 'SPAë¸Œëœë“œ', 'íŒ¨ìŠ¤íŠ¸íŒ¨ì…˜'
        ]
        
        # ì‹ ë¢°í•  ë§Œí•œ ë‰´ìŠ¤ ì†ŒìŠ¤ë“¤
        self.trusted_sources = [
            'ì—°í•©ë‰´ìŠ¤', 'ë‰´ì‹œìŠ¤', 'ë‰´ìŠ¤1', 'í—¤ëŸ´ë“œê²½ì œ', 
            'í•œêµ­ê²½ì œ', 'ë§¤ì¼ê²½ì œ', 'íŒŒì´ë‚¸ì…œë‰´ìŠ¤', 'ì•„ì‹œì•„ê²½ì œ',
            'íŒ¨ì…˜ë¹„ì¦ˆ', 'íŒ¨ì…˜ì¸ì‚¬ì´íŠ¸', 'í•œêµ­ì„¬ìœ ì‹ ë¬¸', 'ì–´íŒ¨ëŸ´ë‰´ìŠ¤',
            'WWDì½”ë¦¬ì•„', 'ì„¬ìœ ì €ë„', 'íŒ¨ì…˜ì±„ë„'
        ]
        
        # ì œì™¸í•  í‚¤ì›Œë“œ (ê´‘ê³ ì„±, í™ë³´ì„± ë‚´ìš©)
        self.exclude_keywords = [
            'í• ì¸', 'ì„¸ì¼', 'ì¿ í°', 'ì´ë²¤íŠ¸', 'í”„ë¡œëª¨ì…˜', 
            'ê´‘ê³ ', 'í˜‘ì°¬', 'PR', 'í™ë³´'
        ]

    def is_valid_news(self, title, description=""):
        """ë‰´ìŠ¤ê°€ ìœ íš¨í•œì§€ í™•ì¸ (ê´‘ê³ ì„± ë‚´ìš© ì œì™¸)"""
        content = f"{title} {description}".lower()
        
        # ì œì™¸í•  í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì œì™¸
        if any(exclude_word in content for exclude_word in self.exclude_keywords):
            return False
            
        # ë„ˆë¬´ ì§§ì€ ì œëª© ì œì™¸
        if len(title) < 10:
            return False
            
        return True

    def search_naver_news(self, keyword, display=5):
        """ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ (ê°œì„ ëœ ë²„ì „)"""
        url = "https://openapi.naver.com/v1/search/news.json"
        headers = {
            'X-Naver-Client-Id': self.naver_client_id,
            'X-Naver-Client-Secret': self.naver_client_secret
        }
        
        # ìµœê·¼ 3ì¼ ë‚´ ë‰´ìŠ¤ë§Œ ê²€ìƒ‰í•˜ë„ë¡ ê°œì„ 
        three_days_ago = (datetime.now() - timedelta(days=3)).strftime('%Y%m%d')
        
        params = {
            'query': f'{keyword} after:{three_days_ago}',
            'display': display,
            'start': 1,
            'sort': 'date'  # ìµœì‹ ìˆœ ì •ë ¬
        }
        
        try:
            response = requests.get(url, headers=headers, params=params, timeout=10)
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 429:
                logger.warning("API í˜¸ì¶œ ì œí•œ ë„ë‹¬, ì ì‹œ ëŒ€ê¸°í•©ë‹ˆë‹¤...")
                time.sleep(1)
                return None
            else:
                logger.warning(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜({keyword}): {response.status_code}")
                return None
        except requests.exceptions.Timeout:
            logger.warning(f"ë‰´ìŠ¤ ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ({keyword})")
            return None
        except Exception as e:
            logger.warning(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜({keyword}): {e}")
            return None

    def scrape_fashion_sites(self):
        """íŒ¨ì…˜ ì „ë¬¸ ì‚¬ì´íŠ¸ì—ì„œ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ (ê°œì„ ëœ ë²„ì „)"""
        news_list = []
        
        # 1. íŒ¨ì…˜ë¹„ì¦ˆ ë‰´ìŠ¤
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get('https://fashionbiz.co.kr/news/', headers=headers, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ë‰´ìŠ¤ ì•„ì´í…œ ì°¾ê¸° (ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
                articles = soup.select('li.list-item')[:3]  # ìµœì‹  3ê°œ
                
                for article in articles:
                    title_elem = article.select_one('h3 a') or article.select_one('.title a')
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        link = title_elem.get('href', '')
                        if link a
